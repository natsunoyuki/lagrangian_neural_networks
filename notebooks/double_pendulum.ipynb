{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from jax import jit\n",
    "from jax.example_libraries.stax import Dense, Softplus, Tanh, elementwise, Relu\n",
    "\n",
    "from jax.tree_util import tree_flatten\n",
    "from copy import deepcopy\n",
    "\n",
    "from functools import partial \n",
    "\n",
    "from jax.example_libraries import stax\n",
    "from jax.example_libraries import optimizers\n",
    "\n",
    "import tqdm\n",
    "\n",
    "\n",
    "from lagrangian_nns.lnn import raw_lagrangian_eom\n",
    "from lagrangian_nns.utils import wrap_coords\n",
    "from lagrangian_nns.experiment_dblpend.data import get_trajectory_analytic\n",
    "from lagrangian_nns.experiment_dblpend.physics import analytical_fn\n",
    "from lagrangian_nns.lnn import custom_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectView(object):\n",
    "    def __init__(self, d): \n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ObjectView({\n",
    "    'dataset_size': 200,\n",
    "    'fps': 10,\n",
    "    'samples': 100,\n",
    "    'num_epochs': 80000,\n",
    "    'seed': 0,\n",
    "    'loss': 'l1',\n",
    "    'act': 'softplus',\n",
    "    'hidden_dim': 600,\n",
    "    'output_dim': 1,\n",
    "    'layers': 3,\n",
    "    'n_updates': 1,\n",
    "    'lr': 0.001,\n",
    "    'lr2': 2e-05,\n",
    "    'dt': 0.1,\n",
    "    'model': 'gln',\n",
    "    'batch_size': 512,\n",
    "    'l2reg': 5.7e-07,\n",
    "})\n",
    "\n",
    "rng = jax.random.PRNGKey(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vfnc = jax.jit(jax.vmap(analytical_fn, 0, 0))\n",
    "vget = partial(jax.jit, backend='cpu')(jax.vmap(partial(get_trajectory_analytic, mxsteps=100), (0, None), 0))\n",
    "\n",
    "minibatch_per = 2000\n",
    "batch = 512\n",
    "\n",
    "best_params = None\n",
    "best_loss = np.inf\n",
    "\n",
    "@jax.jit\n",
    "def get_derivative_dataset(rng):\n",
    "    y0 = jnp.concatenate([\n",
    "        jax.random.uniform(rng, (batch*minibatch_per, 2))*2.0*np.pi,\n",
    "        (jax.random.uniform(rng+1, (batch*minibatch_per, 2))-0.5)*10*2\n",
    "    ], axis=1)\n",
    "    \n",
    "    return y0, vfnc(y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = get_derivative_dataset(rng)[0][:1000], get_derivative_dataset(rng)[1][:1000]\n",
    "print(batch_data[0].shape, batch_data[1].shape)\n",
    "\n",
    "plt.figure(figsize = [10, 2])\n",
    "plt.plot(batch_data[0][:, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = jit(lambda x: 1/(1+jnp.exp(-x)))\n",
    "swish = jit(lambda x: x/(1+jnp.exp(-x)))\n",
    "relu3 = jit(lambda x: jnp.clip(x, 0.0, float('inf'))**3)\n",
    "Swish = elementwise(swish)\n",
    "Relu3 = elementwise(relu3)\n",
    "\n",
    "def extended_mlp(args):\n",
    "    act = {\n",
    "        'softplus': [Softplus, Softplus],\n",
    "        'swish': [Swish, Swish],\n",
    "        'tanh': [Tanh, Tanh],\n",
    "        'tanh_relu': [Tanh, Relu],\n",
    "        'soft_relu': [Softplus, Relu],\n",
    "        'relu_relu': [Relu, Relu],\n",
    "        'relu_relu3': [Relu, Relu3],\n",
    "        'relu3_relu': [Relu3, Relu],\n",
    "        'relu_tanh': [Relu, Tanh],\n",
    "    }[args.act]\n",
    "    hidden = args.hidden_dim\n",
    "    output_dim = args.output_dim\n",
    "    nlayers = args.layers\n",
    "    \n",
    "    layers = []\n",
    "    layers.extend([\n",
    "        Dense(hidden),\n",
    "        act[0]\n",
    "    ])\n",
    "    for _ in range(nlayers - 1):\n",
    "        layers.extend([\n",
    "            Dense(hidden),\n",
    "            act[1]\n",
    "        ])\n",
    "        \n",
    "    layers.extend([Dense(output_dim)])\n",
    "    \n",
    "    return stax.serial(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_random_params, nn_forward_fn = extended_mlp(args)\n",
    "\n",
    "_, init_params = init_random_params(rng+1, (-1, 4))\n",
    "\n",
    "rng += 1\n",
    "model = (nn_forward_fn, init_params)\n",
    "opt_init, opt_update, get_params = optimizers.adam(args.lr)\n",
    "opt_state = opt_init([[l2/200.0 for l2 in l1] for l1 in init_params])\n",
    "\n",
    "\n",
    "def learned_dynamics(params):\n",
    "    @jit\n",
    "    def dynamics(q, q_t):\n",
    "        state = wrap_coords(jnp.concatenate([q, q_t]))\n",
    "        return jnp.squeeze(nn_forward_fn(params, state), axis=-1)\n",
    "    return dynamics\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def loss(params, batch, l2reg):\n",
    "    state, targets = batch#_rk4\n",
    "    leaves, _ = tree_flatten(params)\n",
    "    l2_norm = sum(jnp.vdot(param, param) for param in leaves)\n",
    "    preds = jax.vmap(\n",
    "        partial(\n",
    "            raw_lagrangian_eom,\n",
    "            learned_dynamics(params)))(state)\n",
    "    return jnp.sum(jnp.abs(preds - targets)) + l2reg*l2_norm/args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_derivative(i, opt_state, batch, l2reg):\n",
    "    params = get_params(opt_state)\n",
    "    param_update = jax.grad(\n",
    "            lambda *args: loss(*args)/len(batch),\n",
    "            0\n",
    "        )(params, batch, l2reg)\n",
    "    params = get_params(opt_state)\n",
    "    return opt_update(i, param_update, opt_state), params\n",
    "\n",
    "\n",
    "best_small_loss = np.inf\n",
    "(nn_forward_fn, init_params) = model\n",
    "iteration = 0\n",
    "total_epochs = 300\n",
    "minibatch_per = 2000\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "lr = 1e-5 #1e-3\n",
    "\n",
    "\n",
    "final_div_factor = 1e4\n",
    "\n",
    "#OneCycleLR:\n",
    "@jax.jit\n",
    "def OneCycleLR(pct):\n",
    "    #Rush it:\n",
    "    start = 0.2 #0.2\n",
    "    pct = pct * (1-start) + start\n",
    "    high, low = lr, lr / final_div_factor\n",
    "    scale = 1.0 - (jnp.cos(2 * jnp.pi * pct) + 1)/2\n",
    "    \n",
    "    return low + (high - low)*scale\n",
    "    \n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.adam(OneCycleLR)\n",
    "\n",
    "init_params = custom_init(init_params, seed=0)\n",
    "\n",
    "opt_state = opt_init(init_params)\n",
    "bad_iterations = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_losses = []\n",
    "opt_state, params = update_derivative(0.0, opt_state, batch_data, 0.0)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    num_samples = 0\n",
    "    all_batch_data = get_derivative_dataset(rng)\n",
    "    \n",
    "    for minibatch in tqdm.tqdm(range(minibatch_per)):\n",
    "        fraction = (epoch + minibatch / minibatch_per) / total_epochs\n",
    "        batch_data = (\n",
    "            all_batch_data[0][minibatch*batch:(minibatch+1)*batch], \n",
    "            all_batch_data[1][minibatch*batch:(minibatch+1)*batch]\n",
    "        )\n",
    "        rng += 10\n",
    "        opt_state, params = update_derivative(fraction, opt_state, batch_data, 1e-6)\n",
    "        cur_loss = loss(params, batch_data, 0.0)\n",
    "        epoch_loss += cur_loss\n",
    "        num_samples += batch\n",
    "\n",
    "    closs = epoch_loss / num_samples\n",
    "    epoch_losses.append(closs)\n",
    "    print('epoch={:04d} lr={:.6f} loss={:.3f}'.format(epoch+1, OneCycleLR(fraction), closs))\n",
    "\n",
    "    if closs < best_loss:\n",
    "        best_loss = closs\n",
    "        best_params = [[deepcopy(jax.device_get(l2)) for l2 in l1] if len(l1) > 0 else () for l1 in params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
